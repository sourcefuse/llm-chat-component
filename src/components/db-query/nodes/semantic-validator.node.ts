import {PromptTemplate} from '@langchain/core/prompts';
import {RunnableSequence} from '@langchain/core/runnables';
import {LangGraphRunnableConfig} from '@langchain/langgraph';
import {inject} from '@loopback/context';
import {service} from '@loopback/core';
import {graphNode} from '../../../decorators';
import {IGraphNode, LLMStreamEventType} from '../../../graphs';
import {AiIntegrationBindings} from '../../../keys';
import {LLMProvider} from '../../../types';
import {stripThinkingTokens} from '../../../utils';
import {DbQueryAIExtensionBindings} from '../keys';
import {DbQueryNodes} from '../nodes.enum';
import {DbSchemaHelperService} from '../services';
import {DbQueryState} from '../state';
import {EvaluationResult} from '../types';

@graphNode(DbQueryNodes.SemanticValidator)
export class SemanticValidatorNode implements IGraphNode<DbQueryState> {
  constructor(
    @inject(AiIntegrationBindings.SmartLLM)
    private readonly llm: LLMProvider,
    @service(DbSchemaHelperService)
    private readonly schemaHelper: DbSchemaHelperService,
    @inject(DbQueryAIExtensionBindings.GlobalContext, {optional: true})
    private readonly checks?: string[],
  ) {}

  prompt = PromptTemplate.fromTemplate(`
<instructions>
You are an AI assistant that judges whether the generated and syntactically verified SQL query will satisfy the user's query and the additional checks provided.
The query has already been validated for syntax and correctness, so you only need to check if it satisfies the user's query and all the additional checks provided.
</instructions>

<latest-query>
{query}
</latest-query>

<user-question>
{prompt}
</user-question>

<database-schema>
{schema}
</database-schema>

{checks}

{feedbacks}

<output-instructions>
If the query is valid and will satisfy the user's query, then return valid, else return invalid followed by the reason why it is invalid.
The format in case of invalid query should be -
invalid: <reason>

The format in case of valid query should just be the string 'valid' with no other explanation or string, the output should just be -
valid
</output-instructions>
<valid-output-example>
valid
</valid-output-example>
<invalid-output-example>
invalid: the query does not follow the additional checks provided
</invalid-output-example>
`);

  feedbackPrompt = PromptTemplate.fromTemplate(`
<feedback-instructions>
We also need to consider the users feedback on the last attempt at query generation.

But was rejected by validator with the following errors -
{feedback}

Keep these feedbacks in mind while validating the new query.
</feedback-instructions>`);

  async execute(
    state: DbQueryState,
    config: LangGraphRunnableConfig,
  ): Promise<DbQueryState> {
    config.writer?.({
      type: LLMStreamEventType.ToolStatus,
      data: {
        status: `Verifying if the query fully satisfies the user's requirement`,
      },
    });
    const chain = RunnableSequence.from([this.prompt, this.llm]);
    const output = await chain.invoke({
      query: state.sql,
      prompt: state.prompt,
      checks: [
        `<must-follow-rules>`,
        'It is really important that the query follows all the following context information -',
        ...(this.checks ?? []),
        ...this.schemaHelper.getTablesContext(state.schema),
        `</must-follow-rules>`,
      ].join('\n'),
      schema: this.schemaHelper.asString(state.schema),
      feedbacks: await this.getFeedbacks(state),
    });
    const response = stripThinkingTokens(output);

    const lastLine = response.split('\n').pop() ?? '';
    const isValid = lastLine.startsWith('valid');
    if (isValid) {
      return {
        ...state,
        feedbacks: state.feedbacks?.filter(
          // remove interal feedbacks generated by validators
          feedback => !feedback.startsWith('Query Validation Failed'),
        ),
        status: EvaluationResult.Pass,
      };
    } else {
      const reason = lastLine.replace('invalid: ', '').trim();
      config.writer?.({
        type: LLMStreamEventType.Log,
        data: `Query Validation Failed by LLM: ${reason}`,
      });
      return {
        ...state,
        status: EvaluationResult.QueryError,
        feedbacks: [
          ...(state.feedbacks ?? []),
          `Query Validation Failed by LLM: ${reason}`,
        ],
      };
    }
  }

  async getFeedbacks(state: DbQueryState) {
    if (state.feedbacks?.length) {
      const feedbacks = await this.feedbackPrompt.format({
        feedback: state.feedbacks.join('\n'),
      });
      return feedbacks;
    }
    return '';
  }
}
